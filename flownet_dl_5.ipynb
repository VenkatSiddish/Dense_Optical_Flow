{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LMd_FyUvUe_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(batchNorm, input_planes, output_planes, kernel_size=3, stride=1):\n",
        "    if batchNorm:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input_planes, output_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "            nn.BatchNorm2d(output_planes),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input_planes, output_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_flow(input_planes):\n",
        "    return nn.Conv2d(input_planes,2,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "\n",
        "\n",
        "def deconv(input_planes, output_planes):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_planes, output_planes, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.1,inplace=True)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "VAwIVNC1Ujqp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlate(inp1, inp2):\n",
        "    out_corr = spatial_correlation_sample(inp1,\n",
        "                                          inp2,\n",
        "                                          kernel_size=1,\n",
        "                                          patch_size=21,\n",
        "                                          stride=1,\n",
        "                                          padding=0,\n",
        "                                          dilation_patch=2)\n",
        "    # collate dimensions 1 and 2 in order to be treated as a\n",
        "    # regular 4D tensor\n",
        "    b, ph, pw, h, w = out_corr.size()\n",
        "    out_corr = out_corr.view(b, ph * pw, h, w)/inp1.size(1)\n",
        "    return F.leaky_relu_(out_corr, 0.1)\n",
        "\n",
        "\n",
        "def crop_like(input, target):\n",
        "    if input.size()[2:] == target.size()[2:]:\n",
        "        return input\n",
        "    else:\n",
        "        return input[:, :, :target.size(2), :target.size(3)]"
      ],
      "metadata": {
        "id": "kvhd-dWWU4TO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
        "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
        "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
        "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
        "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
        "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
        "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
        "        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n",
        "\n",
        "        self.deconv5 = deconv(1024,512)\n",
        "        self.deconv4 = deconv(1026,256)\n",
        "        self.deconv3 = deconv(770,128)\n",
        "        self.deconv2 = deconv(386,64)\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)\n",
        "        self.predict_flow5 = predict_flow(1026)\n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "    def forward(self, x):\n",
        "        out_conv2 = self.conv2(self.conv1(x))\n",
        "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
        "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
        "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
        "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
        "\n",
        "        flow6       = self.predict_flow6(out_conv6)\n",
        "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
        "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
        "\n",
        "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
        "        flow5       = self.predict_flow5(concat5)\n",
        "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
        "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
        "\n",
        "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
        "        flow4       = self.predict_flow4(concat4)\n",
        "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
        "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
        "\n",
        "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
        "        flow3       = self.predict_flow3(concat3)\n",
        "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
        "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
        "\n",
        "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
        "        flow2 = self.predict_flow2(concat2)\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "  "
      ],
      "metadata": {
        "id": "smOJNfLfU8EG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flownets(data=None):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=False)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def flownets_bn(data=None):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=True)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "eno6OStQVAOI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained weights of FlowNetS\n",
        "model_dict = torch.load('/content/flownets_EPE1.951.pth.tar',map_location=torch.device('cpu'))\n",
        "model = flownets(model_dict)\n",
        "# model.flownet\n",
        "# model.load_state_dict(model_dict['state_dict'])\n",
        "\n",
        "frame1 = cv2.imread('img1.jpg',cv2.IMREAD_COLOR)\n",
        "frame2 = cv2.imread('img2.jpg',cv2.IMREAD_COLOR)\n",
        "frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "img1 = torch.from_numpy(frame1.T)\n",
        "img2 = torch.from_numpy(frame2.T)\n",
        "\n",
        "model_input = torch.cat((img1,img2), dim=0)\n",
        "model_input = model_input.to(torch.float32)\n",
        "model_input = model_input.unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    flow = model(model_input)\n",
        "flow_np = flow[0].detach().cpu().numpy().T\n",
        "\n",
        "flow_np = flow_np[:,:,:,0]\n",
        "\n",
        "#flow_lk = cv2.calcOpticalFlowFarneback(frame1_gray, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "flow_lk = cv2.calcOpticalFlowFarneback(frame1_gray, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "# Resize the optical flow to match the original frame size\n",
        "flow_lk = cv2.resize(flow_lk,(flow_np.shape[1],flow_np.shape[0]),interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "mse = np.mean((flow_np-flow_lk)**2)\n",
        "print(mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ToUirBVEbp",
        "outputId": "b5222f28-245a-4a08-f5c0-cca10328f542"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.799227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_HIP8Af2WOYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9feffc3-fc56-4091-cf17-4971f0ab5e29"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/video_data.zip\n",
        "# /content/gdrive/MyDrive/video_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7H_jZI4tgUI",
        "outputId": "1812f579-5039-45d7-c935-1e80b336a7bc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/video_data.zip\n",
            "  inflating: vid13.mp4               \n",
            "  inflating: vid3.mp4                \n",
            "  inflating: vid2.mp4                \n",
            "  inflating: vid11.mp4               \n",
            "  inflating: vid1.mp4                \n",
            "  inflating: vid5.mp4                \n",
            "  inflating: vid14.mp4               \n",
            "  inflating: vid8.mp4                \n",
            "  inflating: vid10.mp4               \n",
            "  inflating: vid12.mp4               \n",
            "  inflating: vid9.mp4                \n",
            "  inflating: vid4.mp4                \n",
            "  inflating: vid15.mp4               \n",
            "  inflating: vid7.mp4                \n",
            "  inflating: vid6.mp4                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ptlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7wcldi6xgFY",
        "outputId": "e138c6e8-8824-4d3e-b6ba-d51d3a7db3b0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ptlflow\n",
            "  Downloading ptlflow-0.2.7-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.1/416.1 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics<=0.9.*,>=0.2\n",
            "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm~=0.6.3\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate~=0.8.3 in /usr/local/lib/python3.9/dist-packages (from ptlflow) (0.8.10)\n",
            "Requirement already satisfied: pillow<=9.2.*,>=5.0 in /usr/local/lib/python3.9/dist-packages (from ptlflow) (8.4.0)\n",
            "Collecting pypng~=0.0.16\n",
            "  Downloading pypng-0.0.21-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops<=0.4.*,>=0.3.0\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests<=2.28.*,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ptlflow) (2.27.1)\n",
            "Collecting pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.12.*,>=1.8.1\n",
            "  Downloading torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<=0.13.*,>=0.9.2\n",
            "  Downloading torchvision-0.12.0-cp39-cp39-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly<=5.9.*,>=5.0.0\n",
            "  Downloading plotly-5.8.2-py2.py3-none-any.whl (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<=1.9.*,>=1.0.0\n",
            "  Downloading scipy-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python<=4.6.*,>=4.0.0.21\n",
            "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<=1.22.*,>=1.17.0\n",
            "  Downloading numpy-1.21.6-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<=1.4.*,>=1.1.0\n",
            "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm<=4.64.*,>=4.41.0\n",
            "  Downloading tqdm-4.63.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<=21.*,>=20.0\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging<=21.*,>=20.0->ptlflow) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas<=1.4.*,>=1.1.0->ptlflow) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas<=1.4.*,>=1.1.0->ptlflow) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly<=5.9.*,>=5.0.0->ptlflow) (8.2.2)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (2023.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (6.0)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 KB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (4.5.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.18.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (2.12.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<=2.28.*,>=2.0.0->ptlflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<=2.28.*,>=2.0.0->ptlflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<=2.28.*,>=2.0.0->ptlflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<=2.28.*,>=2.0.0->ptlflow) (2022.12.7)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas<=1.4.*,>=1.1.0->ptlflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (3.20.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.40.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (1.53.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm~=0.6.3->ptlflow) (3.10.7)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning!=1.3.*,!=1.4.*,<=1.6.*,>=1.1.0->ptlflow) (3.2.2)\n",
            "Installing collected packages: pypng, einops, tqdm, torch, setuptools, pyDeprecate, plotly, packaging, numpy, multidict, frozenlist, async-timeout, yarl, torchvision, torchmetrics, scipy, pandas, opencv-python, huggingface-hub, aiosignal, timm, aiohttp, pytorch-lightning, ptlflow\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.1\n",
            "    Uninstalling setuptools-67.6.1:\n",
            "      Successfully uninstalled setuptools-67.6.1\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.13.1\n",
            "    Uninstalling plotly-5.13.1:\n",
            "      Successfully uninstalled plotly-5.13.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.7.0.72\n",
            "    Uninstalling opencv-python-4.7.0.72:\n",
            "      Successfully uninstalled opencv-python-4.7.0.72\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 59.5.0 which is incompatible.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 einops-0.3.2 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 numpy-1.21.6 opencv-python-4.5.5.64 packaging-20.9 pandas-1.3.5 plotly-5.8.2 ptlflow-0.2.7 pyDeprecate-0.3.1 pypng-0.0.21 pytorch-lightning-1.5.10 scipy-1.8.1 setuptools-59.5.0 timm-0.6.13 torch-1.11.0 torchmetrics-0.8.2 torchvision-0.12.0 tqdm-4.63.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6PRaE9azPs6",
        "outputId": "09494c10-f6cf-4e1f-d6f2-d6d2e49ee53e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FlowNetS(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv3_1): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv4_1): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv5): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv5_1): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv6): Sequential(\n",
              "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (conv6_1): Sequential(\n",
              "    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (deconv5): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (deconv4): Sequential(\n",
              "    (0): ConvTranspose2d(1026, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (deconv3): Sequential(\n",
              "    (0): ConvTranspose2d(770, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (deconv2): Sequential(\n",
              "    (0): ConvTranspose2d(386, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "  )\n",
              "  (predict_flow6): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (predict_flow5): Conv2d(1026, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (predict_flow4): Conv2d(770, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (predict_flow3): Conv2d(386, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (predict_flow2): Conv2d(194, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (upsampled_flow6_to_5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (upsampled_flow5_to_4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (upsampled_flow4_to_3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (upsampled_flow3_to_2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    for param in model.conv1.parameters():\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "RvSQKySa0Jsk"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.applications import FlowNetS\n",
        "# from ptlflow import flownets\n",
        "\n",
        "# Load pre-trained FlowNetS model\n",
        "# flownet_model = flownets(weights='imagenet', include_top=False, input_shape=(384, 512, 6))\n",
        "flownet_model = model\n",
        "\n",
        "# Freeze all layers except for the last few layers\n",
        "for i in range(5):\n",
        "    for param in model.conv1.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "for i in range(5):\n",
        "    for param in model.conv2.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Add new layers for motion estimation\n",
        "x = flownet_model.output\n",
        "x = tf.keras.layers.Conv2D(filters=2, kernel_size=3, padding='same')(x)\n",
        "motion_model = tf.keras.Model(inputs=flownet_model.input, outputs=x)\n",
        "\n",
        "# Compile motion model with mean squared error loss\n",
        "motion_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Collect dataset of videos\n",
        "videos = ['vid1.mp4', 'vid2.mp4', 'vid3.mp4', 'vid4.mp4', 'vid5.mp4', 'vid6.mp4', 'vid7.mp4',\n",
        "          'vid8.mp4', 'vid9.mp4', 'vid10.mp4', 'vid11.mp4', 'vid12.mp4', 'vid13.mp4', 'vid14.mp4',\n",
        "          'vid15.mp4']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_videos = videos[:12]\n",
        "test_videos = videos[12:]\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame = cv2.resize(frame, (512, 384))\n",
        "    return frame\n",
        "\n",
        "train_frames = []\n",
        "for video in train_videos:\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = preprocess_frame(frame)\n",
        "        train_frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "test_frames = []\n",
        "for video in test_videos:\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = preprocess_frame(frame)\n",
        "        test_frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "# Train motion model\n",
        "train_input = np.zeros((len(train_frames) - 1, 384, 512, 6), dtype=np.float32)\n",
        "train_output = np.zeros((len(train_frames) - 1, 384, 512, 2), dtype=np.float32)\n",
        "for i in range(len(train_frames) - 1):\n",
        "    prev_frame = train_frames[i]\n",
        "    next_frame = train_frames[i+1]\n",
        "    train_input[i] = np.dstack((prev_frame, next_frame, np.zeros_like(prev_frame)))\n",
        "    train_output[i] = motion_vectors\n",
        "\n",
        "motion_model.fit(train_input, train_output, batch_size=4, epochs=10)\n",
        "\n",
        "# Test motion model\n",
        "def estimate_motion(frame1, frame2, model):\n",
        "    input_data = np.dstack((frame1, frame2, np.zeros_like(frame1)))\n",
        "    output_data = model.predict(np.array([input_data]))[0]\n",
        "    return output_data\n",
        "\n",
        "def mse(image1, image2):\n",
        "    return np.mean((image1 - image2) ** 2)\n",
        "\n",
        "mse_lk = 0.0\n",
        "mse_bm = 0.0\n",
        "mse_nn = 0.0\n",
        "count = 0\n",
        "for video in test_videos:\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    ret, prev_frame = cap.read()\n",
        "    prev_frame = preprocess_frame(prev_frame)\n",
        "    while True:\n",
        "        ret, next_frame = cap.read\n",
        "        if not ret:\n",
        "            break\n",
        "        next_frame = preprocess_frame(next_frame)\n",
        "\n",
        "    # Compute Lucas-Kanade motion vectors\n",
        "    lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "    motion_vectors_lk, status, err = cv2.calcOpticalFlowPyrLK(prev_frame, next_frame, None, **lk_params)\n",
        "\n",
        "    # Compute block-matching motion vectors\n",
        "    motion_vectors_bm = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Compute deep network motion vectors\n",
        "    motion_vectors_nn = estimate_motion(prev_frame, next_frame, motion_model)\n",
        "\n",
        "    # Compute MSE between motion vectors and ground truth\n",
        "    mse_lk += mse(motion_vectors_lk, motion_vectors)\n",
        "    mse_bm += mse(motion_vectors_bm, motion_vectors)\n",
        "    mse_nn += mse(motion_vectors_nn, motion_vectors)\n",
        "    count += 1\n",
        "\n",
        "    prev_frame = next_frame\n",
        "\n",
        "cap.release()\n",
        "mse_lk /= count\n",
        "mse_bm /= count\n",
        "mse_nn /= count\n",
        "\n",
        "print('MSE Lucas-Kanade: %.4f' % mse_lk)\n",
        "print('MSE Block-Matching: %.4f' % mse_bm)\n",
        "print('MSE Deep Network: %.4f' % mse_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "MPIXMSb8vIbJ",
        "outputId": "89c98529-d2cd-4849-c3ae-53d14d7af60a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-336d5e9f938a>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Add new layers for motion estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflownet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflownet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m   1270\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FlowNetS' object has no attribute 'output'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5emWDvMv0iJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}